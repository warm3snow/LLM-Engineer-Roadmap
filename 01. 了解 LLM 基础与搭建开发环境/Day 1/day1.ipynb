{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 输出维度: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# 定义 Transformer 模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        初始化 Transformer 模型\n",
    "\n",
    "        参数:\n",
    "        input_dim (int): 输入特征维度\n",
    "        nhead (int): 多头注意力头数\n",
    "        hidden_dim (int): 前馈网络的隐藏层维度\n",
    "        num_layers (int): Transformer 编码层数\n",
    "        output_dim (int): 输出维度\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # 定义一个 Transformer 编码层\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=hidden_dim)\n",
    "        # 将多个编码层堆叠成 Transformer 编码器\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # 定义一个全连接层，将 Transformer 的输出映射到最终的输出维度\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "\n",
    "        参数:\n",
    "        x (torch.Tensor): 输入张量，形状为 (seq_len, batch_size, input_dim)\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 输出张量，形状为 (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # 通过 Transformer 编码器处理输入\n",
    "        out = self.transformer(x)  # (seq_len, batch_size, input_dim)\n",
    "        # 对序列维度求平均，聚合序列信息\n",
    "        out = self.fc(out.mean(dim=0))  # (batch_size, input_dim) -> (batch_size, output_dim)\n",
    "        return out\n",
    "\n",
    "# 模型参数\n",
    "input_dim = 10  # 输入特征维度\n",
    "nhead = 2  # 多头注意力头数\n",
    "hidden_dim = 50  # 前馈网络的隐藏层维度\n",
    "num_layers = 2  # Transformer 编码层数\n",
    "output_dim = 1  # 输出维度\n",
    "\n",
    "# 构建模型\n",
    "model = TransformerModel(input_dim=input_dim, nhead=nhead, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "\n",
    "# 示例输入数据：seq_len=5, batch_size=32\n",
    "x = torch.randn(5, 32, input_dim)  # 注意输入的形状 (seq_len, batch_size, input_dim)\n",
    "output = model(x)\n",
    "\n",
    "print(\"Transformer 输出维度:\", output.shape)  # (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 输出维度: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义 RNN 模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        初始化 RNN 模型\n",
    "\n",
    "        参数:\n",
    "        input_dim (int): 每个时间步的输入特征维度\n",
    "        hidden_dim (int): 隐层维度\n",
    "        output_dim (int): 输出维度（如回归任务）\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "\n",
    "        参数:\n",
    "        x (torch.Tensor): 输入张量，形状为 (batch_size, seq_len, input_dim)\n",
    "\n",
    "        返回:\n",
    "        torch.Tensor: 输出张量，形状为 (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        out, _ = self.rnn(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        out = self.fc(out[:, -1, :])  # 提取最后一个时间步的输出\n",
    "        return out\n",
    "\n",
    "# 模型参数\n",
    "input_dim = 10  # 每个时间步的输入特征维度\n",
    "hidden_dim = 20  # 隐层维度\n",
    "output_dim = 1  # 输出维度（如回归任务）\n",
    "\n",
    "# 构建模型\n",
    "model = RNNModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# 示例输入：batch_size=32, seq_len=5\n",
    "# 32个样本，每个样本有5个时间步，每个时间步有10个特征\n",
    "x = torch.randn(32, 5, input_dim)\n",
    "\n",
    "# model(x)  # 前向传播\n",
    "# 运行模型\n",
    "output = model(x)\n",
    "\n",
    "# 注意：RNN 的输入形状为 (batch_size, seq_len, input_dim)\n",
    "# 输出形状为 (batch_size, output_dim)\n",
    "print(\"RNN 输出维度:\", output.shape)  # (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)  # 输出 Transformers 的版本号"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
